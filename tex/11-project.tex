\chapter{PhD project}
%Poznamka pre seba -- toto chcem asi kratsie ako som to zacal pisat. Celkovo by
%o mohlo mat tak par stran

We want to study decoding algorithms for hidden Markov models and pair hidden
Markov models (and their variants) and to apply them in comparative analysis
of biological sequences. We divide our goals into three areas.
\begin{itemize}
\item Study of efficient implementations of existing algorithm for HMMs and
pHMMs, analysis of their expected computational complexity and design of  new
improvements.

\item Study of different decoding algorithms (expressed using the highest
expected gain framework) for HMMs and pHMMs. Analysis of their computational
complexity, and their performance in particular application domains.    

\item Application of pHMMs to improve existing pairwise alignment
tools. Design of new decoding algorithms and pHMMs that take into
account additional informations about structure of biological of sequences.
\end{itemize}
In the following sections we describe problems we want to solve, and propose several
approaches for solving those problems.


\section{Improvements and Analysis of Existing Algorithms}

It has been shown that running time and space of traditional algorithms for working with HMMs can be
improved by various techniques. In section \ref{SECTION:ALGORITHMICIMPROVEMENTS}
we discussed several such techniques, including checkpointing
\cite{Grice1997}, compression \cite{Lifshits2009} or by release of memory which
is no longer necessary \cite{Sramek2007}. We plan to address open equations in
the analysis of such improvements and to propose new techniques.

\subsection{On-line Viterbi algorithm}

In section \ref{SECTION:ONLINEVITERBI} we discussed the  On-line Viterbi
algorithm. The On-line Viterbi algorithm does not store the full matrix of back-links
$B$, instead it keeps a compressed forest
composed of those parts of the state paths that can be in the most
probable state path. As soon as the algorithm detects that some parts cannot be
in the most probable state path, it discards them. The original paper
contains analysis of this algorithm for two-state HMMs. With such
HMM, the expected memory requirements is $O(h\log n)$ where $h$ is a constant
specific for a particular HMM \cite{Sramek2007}. Šrámek {\it et al.} conducted experimental
evaluation of the expected memory complexity and conjectured that the expected
memory complexity of this algorithm is polylogarithmic in the length of the
sequence. However, analytical results for general HMMs are not known.

Our goal is to characterize classes of HMMs for which  the expected memory
complexity of the on-line algorithm will be logarithmic or polylogarithmic in
the length of the input sequence under the assumptions that the input sequence is
sampled from the model or that symbols of the sequence were sampled
independently from certain (for example uniform) distribution.

One possible approach to solve this problem is to search for the ``synchronizing
sequences''.  Sequence $s\in\Sigma^*$ is a synchronizing sequence if there
exists state $v$ and position $p$ in the sequence $s$, such that for all
sequences $x\in \Sigma^*$ and states $u$ the state path $\pi_u$ satisfies
$\pi_u[{p+|x|}]=v$ where $\pi_u$ is the most probable state path of the sequence
$xs$ ending in the state $u$. Intuitively, synchronizing sequence force the
Viterbi algorithm to pass through a certain state and therefore for any sequence
$y\in \Sigma^*$, the most probable state path $\pi_{xsy}$ of sequence $xsy$ has
the same prefix $\pi_{xsy}[:|x|]$. Therefore the On-line Viterbi algorithm after
passing sequence $s$ will print (and discard) the most probable state path for
prefix $x$ of $xs$. 

If an HMM has a synchronizing sequence and $f(n)$ is the expected maximal distance
between two synchronizing strings in a sequence of length $n$ (sampled from
some distribution $D$) then the On-line Viterbi algorithm will have the expected memory
complexity $O(f(n)m)$ where $m$ is the number of states.

Our goal is to develop criterion of the existence of the synchronizing string
for given HMM and to compute an upper bound of the $f(n)$, the expected maximal
distance between two synchronizing strings in the sequence of length $n$. 


\subsection{Mixing Times}

\def\tmix{t_{mix}(\epsilon)}
We propose technique to improve memory complexity of the
Posterior decoding using approximations based on the theory of mixing times.
From HMM $H$ we compute a threshold  $T$ (more details
bellow).  We divide the input sequence $X$ of length $n$ into $l$ overlapping
subsequences $x_i=X[2iT:(2i+3)T]$ (for simplicity we assume that $n=(2l+1)T$).  
We run the Posterior decoding for each subsequence $x_i$ independently, obtaining state path
$\pi_i$. If $m$ is the number of states of $H$
then this will take $O(nm^2)$ time since the total length of sequences $x_i$ is
less than $3n$.  We construct the optimal state path $\pi$ by middle points of
the partial paths $\pi_i$
\[\pi = \pi_0[:2T]
\pi_1[T:2T] \pi_2[T:2T] \dots \pi_{l-2}[T:2T] \pi_{l-1}[T:]\] where
$T=t_{mix}[\epsilon]$.
We conjecture that if $T$ is sufficiently high and the input
sequence $X$ was sampled from $H$, then with hight probability the reconstructed
state path $\pi$ is the state path that maximizes objective function of the
Posterior decoding.  Time complexity of this algorithm is $O(nm^2)$ and memory
complexity is $O(n+Tm)$ which can be considerable improvement over $O(\sqrt n
m)$ algorithm since $T$ is a constant specific for HMM but independent of $n$. However in practice $T$
can be too high.

As a threshold $T$, we can use is the mixing time $t_{mix}$ defined it in the following way:
\[\tmix = \min_{i}\left\{ 
\left(\max_{\mu,\nu,X} || \prob{\pi[i] = \cdot\mid X[:i+1],H_\nu} - \prob{\pi[i] = \cdot\mid
X[:i+1],H_\mu}||_{TV}\right)\leq \epsilon
\right\}
\] 
where $H_\mu$ and $H_\nu$ are HMMs created from $H$ by replacing initial
distribution with $H_\mu$ and $H_\nu$ respectively,
$X$ is a sequence
sampled from $H$ and $||\cdot||_{TV}$ is the total variation distance:
$||\mu-\nu||_{TV}=\frac12\sum_{x\in S}|\mu(x)-\mu(x)|$ where $\mu$ and $\nu$
are probability distribution over a finite set $S$.
Intuitively, after $\tmix$ steps the initial distribution does not matter, since
the total variation of the normalized forward variables $F[\tmix+i,\cdot]$
and the corresponding normalized forward variables of the original computation
of the Forward algorithm is less than $\epsilon$. Normalized forward variables
are forward variables multiplied by a constant so they sum up to one. 
Same holds for the backward variables $B$ and therefore the posterior
probabilities decoded in the middle of subsequence $x_i$ will be almost the same
if we were computing the posterior probabilities on the whole sequence $X$.

Mixing times were originally defined for Markov chains \cite{Levin2006}. Markov
chains are HMMs without emissions: the state path is observed, not hidden. We
want to extend the mixing times for HMMs and find way of estimation $\tmix$ for
a given HMM.

This technique can be also used to reduce memory complexity of HMM training and
to parallelize the Posterior decoding, the Forward algorithm and the
Training algorithms.

\section{Measures for decoding algorithms}

The second area of our interest are different optimization criteria for decoding
HMMs expressed in the form of the highest expected gain framework (section
\ref{SECTION:HEG}) This framework generalizes decoding methods for HMMs. Recall
that decoding method is the method of obtaining an annotation of a sequence from
the sequence and the model. 

In the highest expected gain framework we characterize decoding algorithm by
gain function $f(\Lambda_1,\Lambda_2)$ defined on annotations. Given the
sequence $X$, we seek for an annotation $\Lambda$ of the sequence $X$ with the
highest expected gain \[\Lambda = \arg\max_{\Lambda} E_{\Lambda_X\mid
X,H}[f(\Lambda_X,\Lambda)] =
\sum_{\Lambda_X}f(\Lambda_X,\Lambda)\Pr\left(\Lambda_X\mid X,H\right) \]


Alternative
decoding functions can improve the annotation algorithm as has been shown in
\cite{Nanasi2010,Truszkowski2011} for the HIV virus recombination detection.
We want to study these functions for the virus recombination domain. We
want to explore the  possible extensions by restricting of the structure of the
possible annotation (restriction on the labels that can be in the annotation, or
in general restrict it by a regular expression).

While we plan to further study gain functions for the virus recombination
problem, we want to focus on extensions of the highest expected gain framework to
pair hidden Markov models and develop gain functions (and decoding algorithms
that maximize such functions) tailored to the pairwise alignment problem. 

To extend the highest expected gain framework to pHMMs, we need to extend
definition of an annotation. In general, pHMMs are used for annotation and
comparison of two sequences at once. In comparative gene finding, a pHMM is used
to find genes in both sequences, and actual alignment is not main issue. In
other applications, we are not so much interested in the annotation of the
sequences, but rather in relation between the symbols of the two sequences. A
state path of a pHMM uniquely assigns annotations to the input sequences and
also defines a unique alignment.  Therefore we could define annotations in same
way as for HMMs as a simple mapping from the set of states to the set of labels.
If we do so, it will create the following problem.

Let $H$ be the simple  three state pHMM for sequence alignment from figure
\ref{FIGURE:SIMPLEPHMM}. Let $C=\{M,I\}$ be the set of labels and $\lambda$ be
coloring function, such that $\lambda(M)=M, \lambda(I_X)=I$ and
$\lambda(I_Y)=I$.   For example let $X=Y=ACGT$ and $\Lambda = IIMMII$.  In this
case it is not possible to assign labels to input sequences $X$ and
$Y$ and it is not even possible to assign unique alignment.  Then following $6$
alignments are possible:
\begin{verbatim}
Annotation:    IIMMII    IIMMII    IIMMII    IIMMII    IIMMII    IIMMII
X:             --ACGT    ACGT--    A-CGT-    A-CG-T    -ACGT-    -ACG-T
Y:             ACGT--    --ACGT    -ACG-T    -ACGT-    A-CG-T    A-CGT-
\end{verbatim}
And following three annotations of $X$ and $Y$ are possible: $MMII,IMMI,IIMM$.

Note that the last four  alignments are equivalent -- they only differ in the order
of indels in sequences. All symbols from both $X$ and $Y$ can be aligned to a gap
or to some symbol in other sequence. All symbols from both sequences can be
annotated by $I$ or $M$. Therefore such an annotation does not give us much
information.
For this reason we propose the following the definition of the annotation
function for  pHMMs.

\begin{definition}
Let $H=(\Sigma,V,I,d,e,a)$ be a pHMM and $C=\{c_0,c_1,\dots,c_{l-1}\}$ be the
finite set of labels (or colors). The \firstUseOf{coloring function}
$\lambda:V^*\to C^*$ is function that satisfies following properties: 
\begin{enumerate}
\item $\lambda(v)\in C$ for all $v\in V$.
\item $\lambda(xy) = \lambda(x)\lambda(y)$ for all $x,y\in V^*$.
\item If $\lambda(u)=\lambda(v)$ then $d^x_u=d^x_v$ and $d^y_u=d^y_v$ for all
$u,v\in V$.
\end{enumerate}
Let $X$ and $Y$ be sequences generated by a state path $\pi$. Then the
\firstUseOf{pairwise annotation} of sequences $X$ and $Y$ is 
$\Lambda=\lambda(\pi)$.
\end{definition}

Note that this definition is almost the same as the definition of an annotation
for HMMs (definition \ref{DEFINITION:ANNOTATION}),  
with addition of the  third condition, which ensures that
the annotation keeps information about state duration $d$. Therefore we are able to
derive a unique alignment from an annotation and uniquely assign colors to symbols of
input sequences.

Similarly as with the HMMs, sequences $X$ and $Y$ and pHMM $H$ define
a probability distribution of pairwise annotations $\Lambda$
\[\prob{\Lambda\mid X,Y,H} = \sum_{\pi,\lambda(\pi)=\Lambda}\prob{\pi\mid
X,Y,H}\]

We can also define a gain function as any function $f:C^*\times C^*\to
\mathbb{R}$. In the  highest expected gain problem for pHMM, we search for
the pairwise annotation $\Lambda$ of input sequences $X$ and $Y$ that maximizes
the expected gain:
\[\Lambda = \arg\max_{\Lambda} E_{\Lambda'\mid X,Y,H}[f(\Lambda',\Lambda)]
=  \arg\max_{\Lambda} \sum_{\Lambda'}f(\Lambda',\Lambda)\prob{\Lambda'\mid X,Y,H}\]

Similarly as with HMMs, finding the most probable pairwise annotation (which is equivalent
to finding the most probable state path if $\lambda$ is identity function),
is equivalent to finding the  pairwise annotation with the highest expected gain with
gain function
\[f_{ID}(\Lambda',\Lambda) = \begin{cases}
1 & \text{if $\Lambda'=\Lambda$}\\
0 & \text{otherwise}
\end{cases}\]
To our knowledge, with the pairwise alignment only the Viterbi algorithm (the
most probable state space) or the Posterior decoding was used
\cite{Lunter2008}.

Our goal is to develop and study gain function that can be used for pairwise
alignments. Alignments constructed with such gain function (combined with new
models described in section bellow) should have better quality: fewer biases,
higher sensitivity and lower false positive ratio of the aligned residues.
Of course we have to design gain functions in a way that optimizing them will
be computationally tractable.

One possible extension is to extend the Posterior decoding to more columns: 
the gain function is the number of correctly predicted $k$-tuples of the
alignemnt columns. Other possibility is to address the gap wander and gap attraction
biases. Lunter {\it et al.} reported that the average sequence identity near
gaps was higher then the expected value and with the distance from the gap
the expected sequence similarity lower almost to the correct value. To address
this issue, the gain function should penalize the gaps, that are near other
gaps, by lower gap penalty then the standalone gaps.


\section{Models for Pairwise Sequence Alignment}

Pair hidden Markov models currently used for sequence alignment do not address
all important aspects of the sequence evolution. As we discussed in chapter
\ref{CHAPTER:PAIRHMM}, recent works address different rates of evolution or use
better gap models. However, genome to genome aligners do not use for example the
information about the structure of genes. We want to incorporate similarly into
the model, gene structures as is done in the protein to genome aligners and
comparative gene finders discussed in chapter \ref{CHAPTER:PAIRHMM}.
Specifically, we want to adjust our pHMM to  address the following issue. 

Protein/cDNA to genome aligners and comparative gene
finders use strict models of  gene structure in order to detect genes and
annotate them.  However, in genome alignment we are also interested in parts of
sequences that are not genes, for example pseudogenes. Pseudogenes are former
genes that were disabled during evolution. They have similar structure to genes,
but it can be disrupted in some way (for example missing signals
like start codons, splice sites or others). Therefore our model have to take
into consideration also parts of the sequence that do not conform to the regular gene
structure requirements.

Alongside with the novel features that we want to add to our model, our model
should have also following features: the emission and the transition
probabilities of the model should be parametrized by the evolution distance to
be able to adjust the parameters without the need to train the model on new
organisms. If the resulting model will not be too large, we can address the
problem of the variable substitution rate within one organism similarly as in
\cite{Hudek2010} (described in section \ref{SECTION:FEAST}). Other issue is the
gap model. We want to explore the possibility of inclusion of non-geometric gap
models, that closely approximate the real distribution of gaps. 

Our plan is to combine new decoding algorithm for pHMMs with the new model to
create global alignment tool that will produce alignment with fewer biases and
higher sensitivity than existing tools. After implementation we plan
experimental evaluation on the real data and comparison with with existing
tools. In the case that we will successfully develop better pairwise alignment
tool, it will be possible to incorporate it into the larger pipelines for the
multiple alignment or other bioinformatic research. 

%By improving existing alignments we can  improve the 

\label{LastPage}

