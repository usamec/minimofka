\chapter{Introduction}

\section{Problem of sequence assembly}

DNA sequence of an organism is a string over the four letter alphabet A, C, G, T
(with usual length between millions to billions characters).
Current technologies cannot read the whole sequence at once but produce many
substrings of sequence called fragments or reads (whose positions usually overlaps).
In most cases locations of substrings are sampled using uniform distribution.

The goal of sequence assembly is to reconstruct original string.

One might thought that shortest common superstring (\cite{maier1977note}) 
is a good formalization of~the~problem above. But consider following DNA sequence:

$$AA\textcolor{red}{CGTA}\textcolor{green}{CGTA}\textcolor{blue}{CGTA}GG$$

If our reads have length 4 and come from all possible positions the shortest
common superstring would be:
$$AA\textcolor{red}{CGTA}\textcolor{green}{CGTA}GG$$

In this case we lost one repetitions of repeated sequence. If we assume uniform
sampling from original sequence we might estimate the number of repeated substring
by using coverage information (how many reads cover one position).

But there are also sequences for which we cannot reconstruct the order of elements
in~original sequence, consider the following:

$$AA\textcolor{red}{CTCT}\textcolor{green}{GG}\textcolor{red}{CTCT}\textcolor{blue}{CC}\textcolor{red}{CTCT}TT$$

In this case if we have reads of length 4 we cannot distinguish between sequence above and
sequence:

$$AA\textcolor{red}{CTCT}\textcolor{blue}{CC}\textcolor{red}{CTCT}\textcolor{green}{GG}\textcolor{red}{CTCT}TT$$

Due to this, the goal of the sequence assembly is to reconstruct as long as possible
unambiguous parts of the DNA sequence (called contigs).

\section{Real-life complications}

In practice there are several complications which make sequence assembly even harder.
In following text we mention the major ones.

\subsection{Reverse complement}

Each letter (base) from DNA alphabet has the complementary base. This can
be viewed as homomorphism $h(\cdot)$, where: $h(A) = T, h(C) = G, h(G) = C, h(T) = A$.
The real DNA is composed from two strings: $S$ and $h(S^R)$ -- the reverse complement.
During sequencing process the bases can come from both sides of the DNA and
we do not have information about the side.

\subsection{Diploid genomes}

Lots of organisms have multiple sets of chromosomes (e.g. humans have one set
inherited from mother and other set inherited from father, so each chromosome
is present in two copies). This copies of chromosomes are almost identical but
contain some differences (sometimes only changes in one base, but sometimes also
bigger variations). This poses additional chanllenge since we are trying to recostruct
multiple similar sequences.

\subsection{Errors in reads}

In practice the chemical process of sequencing DNA also produces errors.
Sometimes there are small errors in reads -- i.e. substitutions, insertions
and deletions. The amount of these errors depends on specific sequencing technology
(also some technologies have higher amount of substitutions while other
have high amount of insertions).

There are also reads which do not belong to sequenced genome, but are result
of some contamination.

\subsection{Paired-end reads}

Some technologies start reading fragments from one end and lose accuracy after
reading few hundred bases. To capture information which contains longer part of the
sequence some technologies produce longer fragments from which they readtens to hundreds
of bases from both sides and do not read bases from middle (since there will be
too many errors). The result of this process is a pair of reads for which we know approximate
distance in the DNA sequence. This distance is often called insert size.

\subsection{Sequencing technologies}

We summarize currently used sequencing technologies in \ref{tab:techs}.
This presents additional chalenge for assembling algorithms since they
cannot be tailored only for one technology, but should work with combination
of data from many technologies.

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
Technology & Read length & Error rate & Paired end reads \\\hline
Illumina & 50 - 300 & 2\% & Yes\\\hline
454 & 700 & $0.1\%$ & Yes \\\hline
PacBio & 2000 - 20000 & $14\%$ & No \\\hline
\end{tabular}
\caption{Overview of current sequencing technologies}
\label{tab:techs}
\end{table}

Also note that it is much cheaper to produce paired reads with lower insert sizes,
so it is common to combine multiple libraries such as high coverage reads with low insert size
(e.g. 200 bases), and lower coverage reads with high insert size (e.g. 3000 bases).

\section{Overview of known solutions}

\subsection{Assembly problem formalization}

There are several formulations of the genome assembly problem, but
most of them are not user in practice.

First of them is a variation of shortest common superstring problem, where we
almost account for the possibility of sequencing errors \cite{kececioglu1995combinatorial}. 

\begin{definition}{DNA sequence reconstruction problem 1.}
Given set of reads $\mathcal{F}$ and error rate $\varepsilon$, find
a shortest sequence $S$ such that for every $A \in \mathcal{F}$ there is a substring
$B$ of $S$ such that:
$$min(d(A,B), d(\bar{A}, B)) \leq \varepsilon |A|$$
\end{definition}

This formulation can be also extended to account for contaminated reads and paired reads.
As mentioned above this formulation is problematic since it compresses repeated regions
in the genome.

A better formulation is given by \cite{myers1995toward}. It also considers
coverage of output sequence by substring and wants it to be as uniform as possible.

We consider our reconstructed string $S$ and the layout consisting of
$F$ pairs of integers $(s_i, e_i)$, which indicates starting and ending positions
of reads in the reconstructed sequence. The layout is $\varepsilon$-valid
if for each read $A$ the edit distance between $S[s_i:e_i]$ and the read
is at most $\varepsilon |A|$.

We will now formalize the notion of uniform coverage. Lets consider a observed
distribution of read start points (the proportion of reads which start before $x$):
$$D_{obs}(x) = \frac{|\{s_i < x\}|}{F}$$

We now consider a source distribution of a sampling process $D_{src}$ (which is usually
uniform, but can be nonuniform due to some systematic errors) and define maximum
deviation between these two distributions:
$$\delta = max |D_{obs}(x) - D_{src}(x)|$$

Now we can define DNA sequence reconstruction problem in a better way:

\begin{definition}{DNA sequence reconstruction problem 2.}
Given set of reads $\mathcal{F}$ and error rate $\varepsilon$, find
a sequence $S$ and $\varepsilon$-valid layout which has minimal
maximum deviation between observed and source distrition of reads.
\end{definition}

There are still some ambiguities and problems with this formulation.
One can for example find two solutions which have same maximum deviation
but differ in one base.
Also this formulation can be extended to account for contaminated and paired
reads.

\subsection{Assembly algorithms overview}

Almost all assembly algorithms used in practice are some form of heuristics
without well defined formulation, proof of correctness, etc.
There are well defined algorithms, for example algorithm by \cite{Medvedev2009}
which uses bidirectional flows, but they practical use is very limited (this one
assumes error-free reads).

The goal of assembly algorithms is to "glue" reads which can be unambiguously glued together.
They use efficient representation of overlaps between reads and try to resolve
ambiguous regions using paired end reads and long reads.

The good review of assembly algorithms can be found in \cite{miller2010assembly}.
They can be divided into two types by representation of overlaps they use.

The overlap-layout-consensus algorithms use overlap graph, which directly represents
overlaps between reads (two reads are connected by edge, if there is a sufficiently long overlap between them).
To produce this graph we need to find sufficiently long
overlaps between all reads (and this overlap not need to be exact, we usually allow small edit distance between overlapping parts).
This often leads to algorithms which have quadratic
complexity to the number of reads (but can be often speeded up by various heuristics).
After finding overlaps we usually perform removing of transitive edges (we remove edge
from $u$ to $w$ if there is an vertex $v$ and edges from $u$ to $v$ and from $v$ to $w$).

In the layout step we try to find reasonable layout of reads in the assembly.
Finally, in consensus step we perform base calling, e. g. if 10 reads overlaps in
one possitions and 9 of them says the base should be $A$ and one says $G$ we decide the base to
be $A$.


\medskip

The other algorithms use De Bruijn graphs, which do not work directly with reads, but
with sequences of $k$-bases ($k$-mers). The nodes in De Bruijn graph represent $k$-mers
and edges represent adjacencies between $k$-mers in reads. Note that time
required to construct this graph is linear in the length of reads, but on the other hand
we lose some information since we are not working with whole reads.
