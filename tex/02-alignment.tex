\chapter{Alignments}
\label{CHAPTER:ALIGNMENT}
In this chapter we define alignments, define basic scoring scheme for alignments
and describe basic algorithms that compute biologically relevant alignments of
two sequences. Then we describe several algorithmic improvements and heuristics
that reduce the computational complexity of the sequence alignment algorithms.
In the end we will discuss the possibilities of the extension of the algorithm
to different gap models.

Parts of two sequences are homologous, if they have evolved from same sequence
in their common ancestor. Aim of sequence alignment is to identify homologous
sequences. Sequences can be modified by different evolution events:
mutation of a residue into another residue, deletion of a part of the sequence,
insertion of residues into the sequence. There are also large-scale
rearrangement events like duplications (some subsequence is duplicated and
copied into other part of the sequence), inversions (some subsequence is
reversed) or relocations in which part of the sequence change position.  We will
ignore them for now because they cannot be represented by traditional
alignments. An alignment is a data structure that represents comparisons of two
or more sequences. We obtain an alignment of $k$ sequences by inserting dashes
into individual sequences so that they have the same length. We can represent an
alignment as a matrix or a table. Each row of the alignment is a sequence with
inserted dashes, and each column is list of residues from all rows at the same
position.


An alignment has the following biological meaning: homologous residues (those that
have evolved from a common ancestor) are in same column. Dashes represents
either the parts of sequence that were deleted during evolution (deletions) or
positions where some residues were inserted into some other sequence
(insertions). In an alignment is not possible to distinguish between insertions and
deletions -- we cannot tell if something was inserted into one sequence or if
there was deletion in the other sequences. Therefore we will refer to insertions
and deletions as to \firstUseOf{indels}.

\begin{example} 
Consider the  following evolution history of hypothetical DNA sequences of two
current organisms $X$ and $Y$. There was a parent sequence $P$ which evolved into
$X'$ and $Y'$ and after that $X'$ evolved into $X$ and $Y'$ evolves into $Y$.
These sequences are shown bellow: 
\begin{verbatim}
X:      C C     G C G A C C T T G C             A C C A
X':     C C     G           T T G C             A G C A
P:      A C T G G           T C G C T G A G C T A G C A
Y':     T C T G G           C C           G C T A G C A
Y:      T C T A G           C C           G A T A G C A
\end{verbatim}
%$X$ and $Y$ evolved from parent sequence $P$ through sequences $X'$ and $Y'$.
During evolution from $P$ to $X'$, four events occurred: deletion of 
sequences ``TG''  and ``TGAGCT'' and mutations of two bases. During evolution
from $X'$ to $X$, one base was changed and sequence ``CGACC'' was inserted.  
Similarly during evolution from $P$ to $Y'$, the two bases mutated and two
sequences were deleted. During evolution from $Y'$ to $Y$ one base mutated and
sequence ``CGACC'' was inserted. 

From evolution history described above, we can create an alignment of current
sequences $X$ and $Y$ by removing ancestral sequences $X',Y'$ and $P$, 
%\begin{verbatim}
%X:      C C     G C G A C C T T G C             A C C A
%Y:      T C T A G           C C           G A T A G C A
%\end{verbatim}
%We obtain actual alignment by 
removing columns that contain only gaps and
replacing gaps with dashes (gap symbols). 
\begin{verbatim}
X:      C C - - G C G A C C T T G C - - - A C C A
Y:      T C T A G - - - - - C C - - G A T A G C A
\end{verbatim}
In this alignment, symbols that are in same column are truly homologous (they
evolved from same symbol in $P$).
%If two residues are in same column in alignment then they are homologous.
As you can see, homologous symbols do not have to be equal.
\end{example}

There is only one alignment that reflects the true evolution history. Our goal
is to find this alignment, or at least some alignment, that is as close as
possible. The alignment shown above is a \firstUseOf{global alignment} because it
is alignment of whole sequences $X$ and $Y$. A local alignment is alignment of
parts of sequences: a local alignment of sequences $X$ and $Y$ is a global alignment
of strings $\bar{X}$ and $\bar{Y}$ where $\bar{X}$ is a substring of $X$ and
$\bar{Y}$ is a substring of $Y$.  Since global alignments do not consider
rearrangement events\footnote{Duplication, reversal or
translocation.}, local alignments are useful in such scenarios.  We will mostly
consider global alignments, but most of the methods can be extended also to local
alignments.

In this chapter we will review basic methods for constructing alignments. We will
discuss basic scoring schemes and algorithms that finds optimal alignment under
such scheme.

%At first
%we will discuss pair alignments (alignment of two sequences).
%\correction{Later in this
%chapter we will review some methods how to construct multiple alignments.}{urvat
%ak to nie je pravda}

\section{Scoring Schemes}

Since we want to construct alignments that have biological meaning, we have to
develop a method for assessing the quality of an alignment. One way of doing so
is to define a scoring scheme. Scoring scheme is a method that assigns to every
alignment a real number (called score). The alignments similar to the true
alignment should have higher
score than the alignments that differ from the true alignment. Once we have scoring scheme we will search for an
alignments of the input sequences with the highest score.  There are many ways
how to develop good scoring scheme. 


Scoring schemes covered in this chapter will independently score each column of
an alignment, which does not contain a gap. Gaps will be scored by a penalty
that depends on the length of the gap (the number of consecutive dashes). Score
of an entire alignment is the sum of the scores of all ungapped columns plus the
sum of the scores of all gaps.

In particular
we will assume that all sequences are from a finite alphabet $\Sigma$. For DNA,
$\Sigma=\{A,C,G,T\}$, for proteins $\Sigma$ contains $20$ codes of amino acids.
We will score a column containing residues $a$ and $b$ by $S[a,b]$ where $S$ is
a matrix of size $|\Sigma|\times|\Sigma|$ called \firstUseOf{substitution
matrix}.  A gap of length $x$ has a score $g+dx$, where $g$ is the gap opening
penalty and $d$ is the gap extension penalty. Both are usually negative since we
want alignments that contains many columns with same or similar symbols. With
positive gap penalty there will be tendency towards alignments with many gaps
which reduce the number of aligned residues.  We call this gap scoring scheme an
affine gap model. In general, gaps can be penalized by arbitrary function $f(x)$
(non-affine gap penalties will be discussed in the end of this chapter).

Intuition behind this scoring system is probabilistic. Assume that we have
alignment of $X$ and $Y$ without gaps and that each base in DNA evolves
independently. Therefore for every pair of residues $a,b$ there is probability
that $a$ will evolve into $b$. We denote this probability as $p(a,b)$. Therefore the probability that $X$ will evolve
into $Y$ is product of probabilities that $X_i$ will evolve into $Y_i$
\[
\prob{X\text{ evolved into }Y} = \prod_{i=0}^{|X|-1}p(X_i,Y_i)
\]
If $X$ and $Y$ were generated independently be some random model $R$, then probability that we see $X_i$
and $Y_i$ is $\prob{X_i\mid R}\prob{Y_i\mid R}$ ($R$ usually samples symbols
of sequence independently from some distribution). The probability that we $X$
and $Y$ under model $R$ is
\[
\prob{X,Y\mid R} = \prod_{i=0}^{|X|-1}\prob{X_i\mid R}\prob{Y_i\mid R}
\]
As a measure of  $X$ and $Y$ being homologous we take the ratio of the
probability that $X$ evolved into $Y$ and the probability that $X$ and $Y$ are
independent.
By taking logarithm of this ratio we have
\[
\log\left(
\frac{\prob{X\text{ evolved into }Y}}{\prob{X,Y\mid R} }\right)
= \sum_{i=0}^{|X|-1}\log\left(\frac{p(X_i,Y_i)}{\prob{X_i\mid R}\prob{Y_i\mid
R}}
\right)
\]
Therefore by setting $S[a,b]=\frac{p(a,b)}{\prob{a\mid R}\prob{b\mid R}}$ we have one
possible scoring model. $S[a,b]$ is positive if is more likely that $a$ evolved
into $b$ and negative if it is more likely that $a$ and $b$ were generated
independently.  Values of $S$ are usually probabilities multiplied by some
constant and rounded to integers to avoid use of floating point numbers. 

There are two ways of deriving substitution matrices. One way is to derive them
from alignments that were constructed manually by biologists. Such matrices are
for example PAM or BLOSUM matrices. Other possible solution is to use a
theoretical model of evolution, for example Jukes-Cantor model. More details can
be found in \cite{Durbin1998}.



\section{Needleman-Wunsch algorithm}
\label{SECTION:NEEDLE}


Recall that the higher score mean better alignment so we want to search for an
alignment with the highest score.  Alignment with scoring scheme with previous
section can be found by the Needleman-Wunsch algorithm \cite{Durbin1998}.
This algorithm uses an arbitrary score table $S$, affine gap model with gap penalty
$d$ and gap opening penalty $g=0$ (if $g=0$ then this model is also called
linear gap model). To align sequences $X$ and $Y$ of length $n$ and $m$
respectively, we define matrix $M$ of size $n\times m$. $M[i,j]$ will be the
score of the best alignment of sequences $X[:i]$ and $Y[:j]$. We can compute
$M[i,j]$ by the following equations:

\begin{align} 
M[-1,-1] &= 0\\
M[-1,i] &= i\cdot d, 0< i < m\\\
M[i,-1] &= i\cdot d, 0< i < n\\
M[i,j] &= \max
\begin{cases}
 M[i-1,j-1]+S(X_i,X_j)\\M[i,j-1]+d\\
 M[i-1,j]+d
\end{cases}, 0\leq i<n,0\leq j<m \label{ALIGN:ALGO:AFFINE}
\end{align}

By computing $M[n-1,m-1]$ we have the score of the optimal (highest-scoring) alignment of $X$ and $Y$. 

To cope with gap opening penalty, we have to slightly change the algorithm.
We define two other matrices $M_X$ and $M_Y$ of same size as $M$. $M_X[i,j]$
will contain the highest score of an alignment of sequences $X[:i]$ and $Y[:j]$
that ends
with a gap in sequence $X$. $M_Y$ is analogous. Now we show how to compute
$M,M_X,$
and $M_Y$. 

\begin{align}
M[-1,-1] &= 0\\
M[-1,i] &= M_X[-1,i] = i\cdot d+g, 0 < i < m\\
M[i,-1] &= M_Y[i,-1] = i\cdot d+g, 0 < i < n\\
M_X[i,-1] &= -\infty, 0\leq i< n\\
M_Y[-1,i] &= -\infty, 0 \leq i< m\\
M[i,j] &= \max
\begin{cases}\label{ALIGN:ALGO:REALAFFINESTART}
 M[i-1,j-1]+S(X_i,X_j)\\
 M[i,j-1]+d\\
 M[i-1,j]+d\\
 M_X[i,j]\\
 M_Y[i,j]
\end{cases}, 0\leq i<n,0\leq j<m\\
M_X[i,j] &= \max
\begin{cases}
M[i-1,j]+g+d\\
M_X[i-1,j]+d
\end{cases}, 0\leq i<n,0\leq j<m\\
M_Y[i,j] &= \max
\begin{cases}
M[i,j-1]+g+d\\
M_Y[i,j-1]+d
\end{cases}, 0\leq i<n,0\leq j<m\label{ALIGN:ALGO:REALAFFINEEND}
\end{align}




Now we show how to compute  values of $M$ (and optionally $M_X$ and $M_Y$)
effectively.
Both algorithms from previous sections contain recurrent equations for computing
values of matrix $M$ (or three matrices $M,M_X$ and $M_Y$). To compute the
value of $M[i,j]$, these equations use only values
from neighbouring cells which have at least one coordinate smaller. 
Therefore we can order computation so that when we compute value $M[i,j]$ the
necessary values are already computed.
Let $F$ be
the function, that takes as input matrix $M$  and coordinates $(i,j)$ and computes $M[i,j]$ according equation
\ref{ALIGN:ALGO:AFFINE}.  Let $F'$ be
same function as $F$, but with $\max$ replaced with $\arg\max$.  Function
$F'(M,(i,j))$ will thus return which cell was used to computation of $F(M,(i,j))$.
Note that if $g$ is not zero, then $F$ would take as input the matrices $M,M_X,$
and $M_Y$ and compute computed $M[i,j],M_X[i,j],$ and $M_Y[i,j]$ according 
 equations
\ref{ALIGN:ALGO:REALAFFINESTART}-\ref{ALIGN:ALGO:REALAFFINEEND}
%For simplicity, we will assume that both sequences has same length $n$. This
%will allow us to express complexity in simple way.  At first we show how
%to compute compute those algorithms in $O(n^2)$ time and $O(n^2)$

The Needleman-Wunsch algorithm can be computed by the following code. Note that
if gap opening penalty is not zero, matrix $M$ should be replaced with triple of
matrices $(M,M_X,M_Y)$. 


\lstset{showstringspaces=false}
\vbox{
\begin{lstlisting}
Initialize M[i,-1] and M[-1,i]
for i in 0...n-1
  for j in 0...m-1
    M[i,j] = F(M,(i,j))
(i,j) = (n-1,m-1)
while i > 0 or j > 0
  (i',j') = F'(M,(i,j))
  (a,b) = (X[i],Y[j])
  if i' = i then a = '-'
  if j' = j then b = '-'
  print column of alignment (a,b)
  (i,j) = (i',j')
\end{lstlisting}
}
Lines 2-5 fills matrix $M$ and lines 6-12 implement the back-tracing procedure.
Time complexity of this algorithm is $O(mn)$ and memory requirements are $O(mn)$
since we keep matrix $M$ in memory (same complexity for the algorithm for
affine gap model with $g\not=0$).


Note that for computing  $i$-th row of matrix $M$ we need only values from row
$i$ and $i-1$. Therefore if we want to know just the score of the optimal
alignment,
we can compute it in $O(m+n)$ memory: after computing of row $i$, we can discard
row $i-1$. However if we want find the optimal alignment we have to keep matrix
$M$ in the memory or use one of the techniques that are described in following
chapter.


\section{Dynamic Programming}  
%effectively and describe several methods how to decrease memory footprint and/or
%time complexity. 

%As in previous case, values of $M,M_X$ and $M_Y$ can be computed by simple
%dynamic programming and by back-tracing these matrices we can obtain optimal alignment of
%$X$ and $Y$.  Both algorithms have time complexity of $O(nm)$ and memory
%complexity $O(mn)$.  Using tricks described later, the memory and even time
%complexity will be improved.

In this section review several techniques that can be used to improve
performance of dynamic programming used to compute alignment.

\subsection{Restricting Search Space}

One commonly used technique for speedup (and decreasing memory requirements) of
sequence alignment is to restrict the search space of dynamic programming. We
compute alignment only in some parts of matrix $M$ and we assume that omitted
parts of matrix correspond to alignments with low score. In this section we will
review a few techniques that are used to restrict the search space of local or
global alignment.

If the two sequences are quite similar, the optimal global alignment will not be
too far from the main diagonal of matrix $M$. Therefore it is not necessary to compute
parts of matrix $M$ that are too far from the main diagonal \cite{Chao1992} (distance from
diagonal a is
user-defined value or can be computed during alignment \cite{GusfieldBook}).
However, this method is not useful for local alignment or global alignment of 
distant sequences. Now we will discuss some more advanced techniques to
restricting search space of dynamic programming.



\subsubsection{Seeds}

Seeds are a technique frequently  used to reduce time complexity of local alignment.  They were
introduced in BLAST algorithm \cite{Altschul1990}.  Seed is a short alignment which
is likely to be a part of an alignment with high score. After a seed is
found, it is extended with an extension algorithm to a local alignment.

Seeds that cannot be extended to high-scoring alignments are
discarded. Such candidate seeds are called false-positives.
Alignments that were not found by our heuristics
are called false-negatives.
It is important that heuristics used to find seeds has a small number of
false-negatives and large number of true-positives,
otherwise many true high-scoring local alignments will not be found. On the other
hand, high number of false positives implies longer running time. 


The most traditional approach is to take as seeds all positions $i$ and $j$ in
sequences $X$ and $Y$, such that $X[i:i+\tau]=Y[j:j+\tau]$ for some constant
$\tau$. This approach is used in BLAST \cite{Altschul1990}.  Various
generalization of BLAST heuristic were developed to improve trade off between
speed and accuracy, such as seeds with mismatches \cite{Kent2002}, space seeds
\cite{Ma2002}, vector seeds \cite{Brejova2005vector} or daughter seeds
\cite{Csuros2005}. All of those methods vary in speed and accuracy.

Extension of a seed to a full alignment is done in both directions, usually
using equation \ref{ALIGN:ALGO:AFFINE} (equation is altered to reverse
direction). Extension is stopped, when some criterion is reached. For example,
BLAST introduced X-drop heuristics: extension stops if the score of an alignment
is lower than the best score that was seen so far minus some user-defined
constant \cite{Altschul1997}.

%space seeds, daughter seeds
\subsubsection{Stepping-Stone Algorithm}
\label{SECTION:SSA}

\abbreviation{Stepping-stone algorithm}{SSA} 
\cite{Meyer2002,Pairagon2009} is suitable for global alignment. It uses
a local alignment algorithm as a subprogram. The idea is to use good local
alignments as anchors. An anchor is similar to a seed: it is a short alignment which we
expect to be in the optimal global alignment.  However, local alignment tools
give local alignments that do not have to be consistent with each other. A set of
local alignments is consistent if all local alignments can be together in one
global alignment. Let $A$ and $B$ be a local alignments of sequences $X$ and
$Y$ and $(i,j)$ be the position of the last alignment column of $A$ in the dynamic
programming matrix and $(k,l)$ be the position of the first alignment column of
$B$ in the dynamic programming matrix. Then $A$ and $B$ are consistent if $i<k$
and $j<l$.
Therefore we need to choose a consistent subset of alignments.

To get such subset $S$, SSA uses the following greedy method. It starts with
$S=\emptyset$. In every step, it finds alignment $A$, such that $S\cup\{A\}$ is
a consistent set of alignments, and $A$ has the highest score possible. Once there is no
such alignment, the algorithm will stop, and alignments in  $S$
will be used as anchors.

Unlike seeds, these anchors will not be simply extended to a global alignment. Since
local alignments can contain some errors (they were generated by some faster
local alignment tool), SSA will relax them. If $X_i$ and $Y_j$ were aligned in
some anchor, then $X_i$ can be aligned to positions from $j-\tau$ to $j+\tau$ in
global alignment\footnote{Or aligned to a gap in that region.} for user defined
constant $\tau$. Similarly, $Y_j$ can be aligned to positions from $i-\tau$ to
$i+\tau$.

Time complexity and memory requirements of the stepping-stone algorithm are of
order of $O(\sqrt{|X|^2+|Y|^2})$ under the assumption that the maximum area between
two anchors is independent of the sequence length \cite{Meyer2002}.


\subsection{Checkpointing}

Checkpointing is a general technique in dynamic programming, which allows reduce
the number of rows of the dynamic programming matrix to $O(\sqrt n)$ while
doubling the running time
\cite{Grice1997}. 

In order to compute the $i$-th row of matrix $M$, we need only row $i-1$. As
mentioned in section \ref{SECTION:NEEDLE}, to compute the score of the best
alignment we need to store only
two consecutive rows.  If we want to
recover the optimal alignment, after we have computed its score, we need all
rows of matrix $M$ again, in decreasing order.
A naive algorithm that recompute needed rows from the beginning have
time complexity $O(m^2n^2)$

Checkpointing solves this problem by storing every $k$-th row of matrix $M$,
$\lceil n/k\rceil$ rows in total.  While back-tracing, we
will remember an additional block $B$ of consecutive $k$ rows in interval
$[ik,(i+1)k)$. We can compute such a block in $O(kn)$ time using the basic
 dynamic programming, starting from row number $ik$ which is stored in memory.  If we need row that
is outside $B$, we replace block $B$ with block that contain such row. Since we
access every row only once and in decreasing order, we recompute every block at
most once. Therefore the time complexity of back-tracing will be still $O(mn)$. We
have to remember every $k$-th row and one block of size $k$, so memory
complexity is $O(\lceil n/k\rceil m+ km)$. If we set $k=\sqrt n$ then the memory
complexity will be $O(m\sqrt n)$ while the time complexity will remain same.

 
\subsection{The Hirschberg Algorithm}

The \abbreviation{Hirschberg algorithm}{HA} is a divide and conquer approach to reduce
memory requirements of sequence alignment \cite{Hirschberg1975}. The idea is
the following: if we want an alignment of sequences $X$ and $Y$ that has bases $X_i$
and $Y_j$ aligned, we have to  do dynamic programming only in submatrices
$M_1=M[0:i,0:j]$ and $M_2=M[i:n-1,j:m-1]$. If $i=\lceil n/2\rceil$ then
the total number of cells in those matrices is roughly half of the number of
cells in $M$. The Hirschberg algorithm incorporates procedure how to compute 
$j$ such that $X_i, i=\lceil n/2\rceil$ is aligned to $Y_j$ in optimal alignment of both sequences in $O(nm)$
time and $O(n+m)$ memory. There is one problem. $X_i$ does not have to
be aligned to $X_j$, because $X_i$ is indel. In such case, there is $j$ such
that $X_i$ is aligned to gap that comes right after $X_j$ and we have to change
$M_1$ to $M[0:i,0:j+1]$. HA use such $j$ to determine submatrices $M_1,M_2$.
Optimal alignment is concatenation of optimal alignments in matrices $M_1$ and
$M_2$.

To determine $j$, such that $X_i,i=\lceil n/2\rceil$ is aligned to $Y_j$ (or
$X_i$ is aligned to gap right after $Y_j$) HA uses following algorithm: Let
$B(X,Y)$ be the algorithm, that computes for $X$ and $Y$ vector $LL(k)$, where
$LL(k)=M[n-1,k]$ ($LL$ is the last row of $M$). This can be computed in
$O(nm)$ time and $O(n+m)$ memory using algorithm from section
\ref{SECTION:NEEDLE}.  

We compute a vector $LL_1=B(X[0:i],Y)$. $LL_1[k]$ contains score of the optimal
alignment of $X[0:i]$ and $Y[0,k]$. Let $LL_2[k]$ contains score of the optimal
alignment of $X[i,n]$ and $Y[k,m]$. Then $LL_2=B( (X[i,n])^R,Y^R)^R$.
%which is the last row of the 
%dynamic programming matrix applied to the left part of matrix $M$, and $LL_2=B(
%(X[i,n])^R,Y^R)$, which is the last
%.
%While $LL_1[k]$ contains score of optimal alignment of $X[0:i]$ and $Y[0:k]$,
%$LL_2^R[k]$ contains score of optimal alignment of $X[i,n]$ and $Y[k,m]$.
Searched column $j$ is column that maximizes $\max\{LL_1[j]+d+LL_2[j],
LL_1[j-1]+LL_2[j] \}$.

Since total size of the two subproblems is always at most half of the size of the
original problem, the running time of the algorithm will be roughly double of
the standard dynamic programming.
. Therefore the
running time of HA is still $O(mn)$. HA keeps in memory only a constant number
of rows of $M$ and the reconstructed alignment and therefore the memory requirements are
$O(n+m)$.

The Hirschberg Algorithm (HA) reduce memory more than checkpointing, 
However, checkpointing can be applied to a wider class of algorithm, as we will
see in section \ref{SECTION:HMMCHECKPOINTING}.
%but HA can be
%only applied to algorithms that use back-tracing. For example Forward-Backward
%algorithm can be improved by checkpointing but not by HA. More in chapter
%\ref{CHAPTER:HMM}.



\subsection{Exploiting Sequence Repetition}

This technique reduce time complexity of alignment algorithm to $O(n^2/\log n)$
This idea combines LZ78 factorization \cite{Lempel1976} and
$O(A+B)$ algorithm for computing row minima/maxima in totally a monotone matrix of
size $A\times B$ \cite{Aggarwal1987}. We will discuss only two main ideas behind
this algorithm. 
%This technique is the extension of the four-russian trick to
%unrestricted cost matrices. Four-russian trick use clever precomputation to
%speed up sequence alignment \cite{GusfieldBook}

\subsubsection{Totally Monotone Matrices}

\begin{definition}\cite{Crochemore2002}
A Matrix $M$ of size $n\times m$ is \firstUseOf{totally monotone} (with concave condition),
if and only if for all $0\leq i,j< n, 0\leq k<l<m$ the following condition holds:
if $M[i,k]\leq M[j,k]$ then $M[i,l]\leq M[j,l]$.
\end{definition}

If $M$ is totally monotone, then we can compute maximum of every row 
in $O(n+m)$ time by SMAWK algorithm \cite{Aggarwal1987} (This problem is called
\firstUseOf{row maxima}).

\begin{definition}\cite{Crochemore2002}
Let $M$ be a matrix and $M'$ be its
submatrix. \firstUseOf{Input border} $I_{M'}$ of $M'$ is the left column and top
row of $M'$ and \firstUseOf{output border} $O_{M'}$ of $M'$ is the right column and
bottom row of $M'$. Elements of $I_{M'}$ are ordered in clockwise direction
and elements of $O_{M'}$ are ordered in counter-clockwise direction.
\end{definition}

Intuition behind input and output border is following. If we look on the
computation of alignment algorithm inside submatrix $M'$, input border is input
to this computation and output border is output of this computation.

\begin{definition}\cite{Crochemore2002}
Let $M'$ be submatrix of $M$, $I_{M'}=\{i_0,i_1,\dots,i_{k-1}\}$ be its input
border, and $O_{M'}=\{o_0,o_1,\dots,o_{l-1}\}$ be its output border. Then
matrices
$DIST$ and $OUT$ of size $k\times l$ are defined in following way:
$DIST[a,b]$ is the cost of optimal alignment from cell $i_a$ to cell $o_b$.
$OUT[a,b]=i_a+DIST[a,b]$.
\end{definition}

Clearly, $o_b=\max_{0\leq a < k}OUT[a,b]$. Therefore by computing row maxima in
matrix $OUT$, we can compute values of output borders. Matrices $DIST$ and $OUT$
are totally monotone \cite{Crochemore2002}.  If we have matrices $DIST$ and
$OUT$ in advance and $M'$ is of size $m'\times n'$ then we can compute values of
output border in time $O(n'+m')$ by the SMAWK algorithm.

Now we show how to divide matrix $M$ into several submatrices so that
it is possible to efficiently represent matrices $DIST$ and $OUT$.

%Let $M$ be dynamic programming matrix for alignment algorithm for some sequences
%$X$ and $Y$. Let $M' =
%M[i:j,k:l]$ be rectangular submatrix of $M$. Values of $M'$ depends on 
%values cells in submatrices $M[i-1,j:l]$ and $M[i:k,j-1]$ and on value
%$M[i-1,j-1]$. Let $I_{M'}$ be the set of such cells. Let $O_{M'}$ be the 
%the set of cells in $M[i-1,j:l]$ and $M[i:k,j-1]$ (the cells in the top row or
%right column of $M'$). Let $I_0,I_1,\dots$ be enumeration of cells in $I_{M'}$
%and $O_0,O_1,\dots$ be enumeration of cells in $O_{M'}$.  Now we will construct
%the  matrix $DIST_{M'}$ of size $|I_{M'}|\times|O_{M'}|$ where  
%$DIST_{M'}[i,j]$ is the cost of optimal alignment from cell $I_i$ to cell $O_j$.
%Let $OUT[i,j]=I_i+DIST[i,j]$. Both $DITS$ and $OUT$ are totally monotone with
%convex condition 


\subsubsection{LZ78 factorization}

LZ78 is a compression algorithm that uses a dynamic dictionary, which is being built
while the sequence is compressed \cite{Lempel1976}. The way how it parses the sequence
can be used to accelerate dynamic programming \cite{Crochemore2002,Weimann2009}. 
LZ78 factorization divides
sequence $S$ into $k$ strings $S_0,\dots,S_{k-1}$, where $S_0S_2\dots S_{k-1}=S$ and
for every index  $i,0< i <k$ there is index $0\leq j<i$ such that $S_j$ is
prefix of $S_i$ of size $|S_i|-1$. We will call $S_j$ to be a
\firstUseOf{predecessor} of $S_i$.  We have guarantee that the number of strings
$k$ is in  $O(\frac{n}{\log n})$ where $n$ is the length of $S$
\cite{Lempel1976}. 

To align sequences $X$ and $Y$, we factorize them into sequences of strings
$\{X_i\}_{0\leq i < k_x}$ and $\{Y_i\}_{0\leq i<k_y}$.  Every pair $(X_i,Y_j)$
defines a rectangular submatrix $B_{i,j}$ of $M$.  All $B_{i,j}$ are
disjoint and all blocks cover matrix $M$. We say that block $B_{k,l}$ is
a predecessor of block $B_{i,j}$ if one of the following conditions is true:

%If we want to align sequences $S$ and $T$, we factorize $S$ and $T$
%into sequences of strings $\{S_i\}_{0\leq i < k_s}$ and $\{T_i\}_{0\leq i<
%k_t}$. Every pair $(S_i,T_j)$ defines rectangular block of matrix $M$.
%We say that $(S_k,T_l)$ is predecessor of $(S_i,T_j)$ if one of the following
%conditions is true:

\begin{itemize}
\item $X_k$ is predecessor of $X_i$ and $l=j$
\item $k=i$ and $Y_l$ is predecessor of $Y_j$
\item $X_k$ is predecessor of $X_i$ and $Y_l$ is predecessor of $Y_j$
\end{itemize}

Note that every block has $0,1$ or $3$ predecessors. Only block
$B_{0,0}$ has $0$ predecessors and only blocks $B_{i,0}$ and $B_{0,i}$
($i>0$) have only $1$ predecessor.  

%\todo{Mozno by nebolo odveci popisat aj tu strukturu, ak bude cas} NEBUDE
{\it Crochemore et al.} described a data structure that represents $DIST$ and $OUT$
matrices for  \nocite{Crochemore2002} individual blocks $B_{i,j}$. Structures
for $B_{i,j}$ can be
computed from $DISTS$ and $OUT$ matrices of theirs predecessors in time
$O(|X_i|+|Y_j|)$. Details of this algorithm can be found in
\cite{Crochemore2002}. Time complexity of this algorithm is proportional to the
sum of the sizes of all blocks, which is $O(n^2/\log n)$ where $n$ is the length
of the longer sequence.





\section{Non-Affine Gap Models} 


%\todo{Mozno by sa dalo dodat aj viac citacii} 
The reasons why affine gap models are
used in sequence alignment are that they are simple, easy to compute and give
reasonable results. While affine gap scoring works fine for short gaps, penalty
for long gaps is too high. Use of different gap models can improve quality of
reconstructed alignments \cite{Gill2004,Cartwright2009}.

Let $f(x)$ be the gap penalty for a gaps of length $X$.
In 
affine gap models, this function has the form $f(x)=g+dx$. For general gap function, we have to
reformulate equation \ref{ALIGN:ALGO:AFFINE} for computing the highest scoring
alignment in following way:
\begin{align}
M[i,j] &= \max
\begin{cases}
 M[i-1,j-1]+S(X_i,X_j)\\
 \max_{x\leq j}M[i,j-x]+f(x)\\
 \max_{x\leq i}M[i-x,j]+f(x)
\end{cases}, 0<x\leq i<n\label{ALIGN:ARBITRARYGAPEQUATION}
\end{align}
This algorithm has time complexity of $O(n^3)$, where $n$ is the length of the
longer sequence. Note that this was the original Needleman-Wunsch 
algorithm \cite{Needleman1970}, and later it was improved to $O(n^2)$ algorithm
\cite{Sankoff1972} for alignment with linear gap penalties.
Unlike previous recursive equations, in this algorithm $M[i,j]$ depends not
only on neighbouring cells, but also on all previous cells in the same row and column.
Therefore some memory-saving techniques like Hirschberg algorithm or checkpointing cannot be
used with general gap penalties.


\subsection{Convex/Concave Gap Functions}\label{SECTION:CONVEX}

Arbitrary gap penalties increased running time of the Needleman-Wunsch algorithm
by factor of $O(n)$. However if we place some
restrictions on the  gap penalty function, we can use faster algorithms. In this
section, we
show an algorithm that computes the optimal alignment in $O(nm\log n)$ time if
the gap
penalty is convex or concave. In some cases this algorithm can be improved to
$O(nm)$ time. This algorithm was discovered independently by Miller
{\it et al. (1988)} and Galil {\it et al. (1989)} \nocite{Miller1988,Galil1989}.

We will present a variant of $O(nm\log n)$ algorithm for convex gap functions which
we believe is easiest to explain . Algorithm for concave gap function is similar.
We consider only convex functions defined on natural numbers (gap length can be only
natural number). Our definition of convex function is similar to one used in
\cite{GusfieldBook}.


\begin{definition}
We assume that $f(x)$ is a  function defined on natural numbers. Function $f$
is convex if and only if 
\[f(x+1)-f(x)\geq f(x)-f(x-1)\]
for all $x\in\mathbb{N}$.
\end{definition}
\begin{note}
It is easy to show that if $f$ is convex then $f(x+d)-f(x) \leq f(x+e+d) -
f(x+e)$ for nonnegative $d$ and $e$.
\end{note}

We will improve dynamic programming that computes matrix $M$ using equation
\ref{ALIGN:ARBITRARYGAPEQUATION}. The increase in running time is caused by two
terms: $M[i,j-y]+f(y)$ and
$M[i-x,j]+f(x)$. In dynamic programming from section \ref{SECTION:NEEDLE} we  have
to add additional nested cycles that go through all values $x$ and $y$. We show
a data
structure that can compute $\max_{0\leq y < j}M[i,j-y]+f(y)$ in $O(\log n)$
amortized time.
This data structure can also handle the second problematic term, but we
will describe only the first one. First we change the variable in the
maximization so that we maximize

%$\max_{0\leq y < j}M[i,j-y]+f(y)$ is equivalent to
\begin{equation}
\max_{0\leq k < j}M[i,k]+f(j-k)\label{CONVEX:MAXFUNCTION}
\end{equation}
which is equivalent to the original expression from the equation
\ref{ALIGN:ARBITRARYGAPEQUATION}.  Our data structure is a list $L_j$ containing
values of 
$k$ that can become maximum for future values of $j$. We will try to keep this list as small as
possible. List $L_j$ will be a called \firstUseOf{candidate list} and members of $L_j$
will be called \firstUseOf{candidates}. Rank of candidate $k$ is the value
$G(k,j)=M[i,k]+f(j-k)$. To compute $M[i,j]$ we iterate over values of $k$ in
$L_j$ and find the one with the highest rank.
%$L_j$ is candidate list for computing $M[i,j]$. 
List $L_{j+1}$ will be computed from $L_j$ by adding $j$ and
removing some elements of $L_j$. 
%For simplicity, let $G(k,j) = M[i,k]+f(j-k)$.
%If $k\in L_j$, than \firstUseOf{rank} of $k$ is $G(k,j)$. 
Note that the rank of
candidates will change when we move from $L_j$ to $L_{j+1}$.

We will explain this algorithm in iterative way: we will be adding to algorithm
new features until it will have desired time complexity. 

%Let $L_j$ be the list of such $k$'s. We will call $L_j$ a \firstUseOf{candidate
%list} and members of $L_j$ \firstUseOf{candidates}. $L_j$ will be created from
%$L_{j-1}\cup\{j-1\}$ by removing some elements. Let $G(k,j) =
%M[i,k]+f(j-k)$.  \firstUseOf{Rank} of candidate $k$ is $G(k,j)$.  i
The trivial algorithm includes all values smaller than $j$ in $L_j$.
%We start with $L_j$ containing all $k$ that are smaller then $j$.  We can find
%element with maximal rank by computing rank for all candidates from candidate
%list $L_j$. 
This leads to $O(j)$ time for finding maximum and $O(1)$ for update
from $L_i$ to $L_{i+1}$
%candidate list (we have to add element $j$ to list $L_{j}$ to create
%list $L_{j+1}$). 
This is equivalent to the original Needleman-Wunsch algorithm
\cite{Needleman1970}. 
We will use the following lemma to
decrease size of candidate lists. This lemma and proof is  slightly modified
from 
\cite{GusfieldBook}.


\begin{lemma}\label{OneStrikeAndOut}
If $G(k,j)\leq G(k',j)$ for some $k<k'<j$, then
$G(k,j')\leq G(k',j')$ for all $j'\geq j$. 
\end{lemma}


\begin{proof}

Inequality $M[i,k]+f(j-k)\leq M[i,k']+f(j-k')$ implies that
$M[i,k]-M[i,k']\leq f(j-k')-f(j-k)$. From convexity of $f$ we have
$f(j-k')-f(j-k)\leq f(j'-k')-f(j'-k)$, and therefore
$M[i,k]-M[i,k']\geq f(j'-k')-f(j'-k)$ which 
is equivalent to
$M[i,k]+f(j'-k)\leq M[i,k']+f[(j'-k')$ or $G(k,j')\leq G(k',j')$.

\end{proof}

Using this lemma, we can improve our algorithm.
Once we find out that $G(k,j)\leq
G(k',j)$, for $k<k'$, we can remove $k$ from $L_{j'}, j'\geq j$ without
affecting the result
of the algorithm, because rank of $k$ will remain smaller than rank of $k'$ for
all $j'\geq j$.
When we move from $j$ to $j+1$, we therefore set $L_{j+1}=L_{j}\cup\{j\}$ and
then
we remove from $L_{j+1}$ that candidates that have lower or equal rank than the 
candidate following them in the list.
%when we move from
%$j-1$ to $j$ we add set $L_j=L_{j-1}\cup\{j-1\}$ and we compute $G(k,j)$ for
%all $k\in L_j$ and remove all such $k$ where there exists $k'\in L_j$ such that
%$G(k',j)\geq G(k,j)$. i
This can be done in $O(|L_{j}|)$ time. Candidates in the resulting list,
$L_{j+1}$ will have decreasing rank. Therefore the first item in
$L_{j+1}$ is the maximal one, so we can find it $O(1)$ time.  This change
does not improve the worst-case behaviour of our algorithm, but it might
significantly reduce the  size of $L_j$ in practice. 


Using this algorithm, $L_j$ will be always ordered by rank. However when we move
from $j$ to $j+1$, the ranks will change, and we have to remove some elements 
to restore this property. The algorithm can be improved if we can compute the column in
which two adjacent  candidates will brake the ordering.

%Nice property of $L_j$ is that it's members are ordered by rank. However, 
%when we move to column $j+1$, ranks of candidates will change and order might be
%broken and therefore we have to traverse through whole $L$ to fix that.
\begin{definition}
For $k<k'$, let $H(k,k')=l$ be the minimal $l$ such that $G(k,l)\leq G(k',l)$. If such $l$ does
not exists,  $H(k,k')=\infty$ 
\end{definition}

If the rank of $k$ is greater than the rank of $k'$ then $H(k,k')$ is the first
column $j'$
 where the rank of $k$ is less than or equal to the
rank of $k'$. Candidate $k$ will be removed in column $j'$, or possibly earlier.  Note that
such $l$ can be found using binary search (based on lemma \ref{OneStrikeAndOut})
in $O(\log n)$ time for any convex function. For some convex functions we can
find it analytically in constant time.


We can further decrease the size of the candidate list using the following
lemma.

\begin{lemma}\label{TimeLemma}
Let $k,k',k''\in L_j$ and $k<k'<k''$. If $H(k,k')\geq H(k',k'')$, element $k'$ can be
removed from $L_j$.
\end{lemma}

\begin{proof}
Since candidates in $L_j$ are ordered by rank, $G(k,j)>G(k',j)>G(k'',j)$.
From definition of $H$ we know that 
$G(k',j')\leq G(k'',k')$ for  all $j'\geq H(k',k'')$. Since
$H(k,k')\geq H(k',k'')$, then $G(k,j')\geq G(k',j')$ for all $j'<H(k,k'')$.
Therefore the rank
of $k'$ will never be maximum and $k'$ can be removed.

%and both are in $L_j$ then $k$ has higher rank that $k'$. Therefore
%it is not maximum. $k'$ have in $H(k',k'')$ lower rank and will be removed from
%candidate list (using lemma \ref{OneStrikeAndOut}). However, since $H(k,k')\geq
%H(k',k'')$ rank of $k'$ will never be maximum.
\end{proof}

Now we can formulate the final algorithm. After updating $L_j$ from $L_{j-1}$,
$L_j$ will satisfy the following invariant.
\begin{invariant}\label{LogGapInvariant}
All consecutive elements $k,k'$ in $L_j$ satisfy the following properties:
\begin{enumerate}
\item $k<k'$
\item $G(k,j)>G(k',j)$ (decreasing rank)
\item if $k'$ is not the last element of $L_j$ and $k''$ is the element
following $k'$
in $L$, then $H(k,k')> H(k',k'')$ (increasing ``time'' when consecutive
candidates will brake the ordering of ranks)
\end{enumerate}
\end{invariant}

Since the rank is decreasing, we can find the maximum in $O(1)$ time. Now we show how
to compute $L_{j+1}$ from $L_j$. Let $L$ be our working list,
$L[k]$ be its $k$-th element, and $l$ be the index of the last element of $L$. 
List $L_{j+1}$ can be computed by the following algorithm. 
\begin{enumerate}
\item $L=L_j$
\item $L_{j+1}=L\cup\{j\}$.
\item If $G(L[0],j+1)\leq G(L[1],j+1)$ then remove $L[0]$ from
list (lemma \ref{OneStrikeAndOut}).
\item While $H(L[{l-1}],L[{l}])\leq H(L[l-1],L[l-2])$ or $G(L[l],j+1)\geq
G([l-1],j+1)$, remove $L[l-1]$ from the list (lemma
\ref{TimeLemma} and lemma \ref{OneStrikeAndOut}).
\end{enumerate}

The first element is also the first element that can break order, so we have to
test it (step 3). Since we add candidate $j$, we have to remove from the end of
the list all candidates with smaller rank than $j$ and those that can be removed
based on lemma \ref{TimeLemma} and lemma \ref{OneStrikeAndOut}.  Since both
ranks and times are ordered, we need to remove candidates only from the end of
the list.

It is easy to see that if $L_j$ satisfies the invariant, $L_{j+1}$  satisfy it.
It is also clear that this algorithm will never remove any candidate that will
be the maximum later, which implies the correctness of the algorithm.

The algorithm described above has running time $O(l\log(n))$, but the amortized time
complexity for computing all columns of one row of matrix $M$ is $O(n\log n)$ since every candidate will
be added/removed to/from list at most once. If we can compute function $H$ in
constant time, the time complexity is $O(n)$.

Using this algorithm, we can compute the optimal alignment with convex gap functions in
$O(nm\log n)$ time and in $O(nm)$ time  if $H$ can be computed
in constant time.

There is also a more complicated $O(nm\alpha(n))$ algorithm for this problem
\cite{Klawe1990}, where $\alpha(n)$ is the inverse Ackermann function. This algorithm
is very complex, and we doubt that it is useful in practice. However, we are
not aware of any experimental comparison of these two algorithms.




%tu chcem definiciu convexnej gap funkcie
%algoritmus ktory je v case n^2 log(n)
%algoritmus, ktory bezi v case n^2 alpha(n)
%kubicky algoritmus v worstcase, ale prakticky konstantny (budem musiet najst 
%citaciu -- nepodarilo sa mi to najst. Zaujimave\ldots)


