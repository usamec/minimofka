\chapter{Evaluating Quality of Assemblies}

As shown in earlier chapters assembly algorithms are mostly
heuristics with almost zero guaranties. 
Result of the assembly are usually input data into several next steps
of the genome analysis (like finding genes, calculating evolutionary history, ...).
Because of this, any errors in assembly process will have impact on further analysis.
Also sometimes we have access to several variants of the assembly and we need
to choose the best one.
Thus it is important to evaluate quality of the resulting assembly.

Evaluating quality of an assembly is usually hard problem,
since we do not have access to the correct answer.
Due to this we usually resort to use inaccurate indicators, which
give as at least some idea about assembly quality.

In this chapter we give overview of several techniques and statistics
for evaluating assembly. We start with simple indicators based
only on assembly. We also describe indicators based on assembly and correct
answer (which we have if we are benchmarking assemblers). 
There are lots of statistics based on mapping reads to assembly. We first give
overview on mapping reads to assembly and then describe REAPR, which is a tool
for evaluating assembly. We finish chapter with description
of probabilistic models for sequence assembly.

\section{Basic Statistics}

\subsection{Statistics Based Only on Assembly}

To evaluate continuity of the assembly, it is vital
to look into distribution of length of contigs.

There are many ways to look at the distribution of contigs.
We give a brief overview of metrics used in QUAST (\cite{Quast}), which
is a standard tool for evaluation of assemblies.

First, we usually look at total length of contigs. We can have expected assembly 
length from other sources so if we the total length of contigs is far from expected
value, we can already see a problem with assembly.
Number of contigs gives us approximate notion of fragmentation of assembly.
Sometimes it is better to look at number of contigs longer than $x$ -- where usually $x = 1000$,
since very small contigs are usually some artefact of the assembly process.
Other statistics for contig length distribution include:
length of the largest contig and $Nx$ (where $0 < x \leq 100$):
the largest contig length $L$, such that using contigs
of length $\geq L$ account for at least $x \%$ of the bases of the assembly.

Note that each of this statistics can be "gambled" so comparing assemblies based
on this statistics can be only done when we assume, that assembly software does only
reasonable operations. This often happens in practice, for example
\citet{gage} shows in his experiments that some assemblers have
higher N50 but also higher number of errors. 

\subsection{Statistics based on assembly and reference sequence}

Sometimes (especially during evaluation of assembly algorithms) we have access to 
the true sequence
(or almost true sequence), and we can compute various statistics which tell us, how many
errors are in our assembly.
Calculating these statistics usually starts with aligning the assembly to the
reference genome, which
gives us information in the form:
"Substring of the assembly starting at position $a$ and ending at position $b$ can be mapped
to the substring of the reference genome starting at $c$ and ending at $d$ with $e$ edits."
We called these aligned substrings blocks. 

\begin{itemize}
\item No. of missasemblies -- missassembly is usually defined as a positions where
positions of block aligned on the left and block aligned on the right differ in reference
by more that $1000$ bases or this block are on opposite strands of in different chromosomes.
\item No. of missassembled contigs -- nubmer of contigs which contain missassembly.
\item No. of mismatches and indels -- these statistics are interesting in one base differences,
either substitutions or insertions and deletions.
\item $NAx$ -- similar to $Nx$, but before computing this statistic we break
contigs at missassemblies.
\end{itemize}

\section{Read mapping}

Many techniques for evaluating assembly rely on finding
positions of original reads in the assembly. This process is often called read mapping.
In this section we will give an overview of read mapping and algorithms and tools used for it.

The goal of read mapping is to find positions in DNA sequence where the read might be sequenced
from. We are usually trying to find positions with as smallest edit distance as possible
(but sometimes we use more complicated metrics, due to prevalence some types of errors in reads).
Formally we can state that given DNA sequence $S$ and read $R$, the goal of read mapping
is to find substring $T$ of $S$ such that edit distance between $R$ and $T$ is as small as possible.

The exact algorithms for read mapping are variants of classical dynamic programming algorithms
for finding edit distance of two strings, like Smith-Watterman algorithm (\cite{sm}).
Basically we are calculating numbers $T[i][j]$ where $T[i][j]$ is a lowest possible edit
distance between some suffix of $S[1..i]$ and $R[1..j]$. 
Unfortunatelly, finding mapping for one read using this algorithm takes $O(|S||R|)$ time,
which is too much considering that DNA sequence has length at millions and we usually have
few million reads.

Practical algorithms for read mapping can be divided into two types:
Hash based seed and extend algorithms and algorithms based on suffix arrays and their extensions.

Hash based algorithms are based on observation that with high
probability some substring of read of length at least $k$ (where $k$ is somewhere
between 13 and 20) is conserved without errors. These conserved substring
are called seeds. 
We can index the DNA seqeunce and then find all possible seeds matches for read.
This is usually done using hash tables. After finding seeds we do
dynamic programming in small area around seed location.
Examples of tools using this approach 
include BLAST, BLAT and many others \citep{blast,blat}.

Other group of read mappers uses extension of suffix arrays called FM-index
\citep{fmindex, fmindex2}, which we will now describe:

\paragraph{Burrows-Wheeler transform.} Given sequence $S = s_1 s_2 \dots s_n$, where
$s_n = \$$. Burrows-Wheeler transform (BWT) of $S$ is constructed
by lexicographically ordering all cyclical rotations of $S$ and then
taking last characters from each rotations.
Formally if $S_i$ is rotation starting at $i$-th positions of $S$ and array
$X_i$ is index of $i$-th lexicographically smallest rotation, the
BWT of $S$ is sequence $B = S_{X_1}[n] S_{X_2}[n] \dots S_{X_n}[n]$.
Using standard algorithms for construction of suffix array we
can construct BWT of $S$ in $O(|S|)$ time and space.
TODO obrazok

\paragraph{Indexing using BWT.}
BWT can be used to find occurrence of string $P$ in $S$ as a substring.
Denote $C(a)$ as a smallest index $i$ where $S_{X_i}[0] = a$ (it is also
a total number of characters smaller than $a$ in $S$).
Denote $Occ(a,i)$ as as number of occurences of $a$ in $B[1], \dots, B[i]$.
The goal of the indexing is to find and interval $(b, e)$ such that
$b$ is the smallest number where $P$ is a prefix of $S_{X_b}$
and $e$ is the largest number where $P$ is a prefix of $S_{X_e}$.
If we know interval $(b, e)$ for $P$, then the interval for
$aP$ is given as:
$$(C(a) + Occ(a, b - 1) + 1, C(a) + Occ(a, e))$$

Using this formula we can calculate interval for any pattern (note that
the interval for empty string is $(1, |S|)$). We can also implement
backtracking to allow inexact matching of the pattern. 
Improved algorithms for inexact matching are described by \citet{fmindex3}.

To achieve good space complexity we use compressed represenation
of arrays $B$, $Occ$ and $C$ and some samples from array $X$ (we can reconstruct
array $X$ using samples and data from $B$).

Example of FM-index based read mappers are Bowtie2, BWA and SOAP
\citep{bowtie2,fmindex,soap}.

\section{REAPR}

If we have access to original reads and the assembly we can find locations
of this reads in the assembly and then compute several statistics which
can point out to suspicious regions in the assembly.

There are several tools which do this, one of them is REAPR (\cite{Reapr}) which we describe
in more detail below:

REAPR is designed for working with paired reads, so in the following we will assume that
we have one library of paired reads. REAPR starts by mapping reads to the assembly, i.e. for each read we know locations it the assembly
and the edit distance for this mapping.

After mapping step we can calculate following metrics for each base in the assembly:
\begin{itemize}
\item Read coverage -- how many mapping of reads overlap current position. Low coverage usually indicates assembly error.
\item Type of read coverage -- we can calculate read coverage for specific conditions such as: reads without pairs, reads with pair with wrong orientation,
coverage for reads which map with or without reverse complementing.
\item Read clipping -- sometimes read can be mapped to assembly but a few bases at the end of the reads do not match. We can reads mapped this way as soft-clipped.
At each base we count the number of soft-clipped reads which mapping starts or end at this base. A high read clipping count can be sign of insertion of deletion
in the assembly.
\item Paired coverage -- same as read coverage but we treat properly paired reads as a one long read when calculating coverage.
\item FCD error -- TODO
\end{itemize}

REAPR uses this metrics to calculate score at each base and then breaks assembly at places with low score.

\section{Probability models}
\label{sec:prob}
In some cases we have to compare multiple assemblies and decide which one is
the best. This can be done using various criteria, but \cite{Ghodsi2013} shows
that very simple and theoretically sound probalistic model can do the job.

In general, the probabilistic model defines the probability $\Pr(R|A)$ that a set of
sequencing reads $R$ is observed assuming that assembly $A$ is the
correct assembly of the genome. Since the sequencing itself is a
stochastic process, it is very natural to characterize concordance of
reads and an assembly by giving a probability of observing a particular
read.

\def\LAP{\mathrm{LAP}}

\paragraph{Basics of the likelihood model.}
The model assumes
that individual reads are independently sampled, and thus the overall
likelihood is the product of likelihoods of the reads:
$\Pr(R|A) = \prod_{r\in R} \Pr(r|A).$
To make
the resulting value independent of the number of reads in set 
$R$, we use as the main
assembly score the log average probability of a read computed as
follows: $\LAP(A|R) = (1/|R|)\sum_{r\in R} \log \Pr(r|A).$ Note that
maximizing $\Pr(R|A)$ is equivalent to maximizing $\LAP(A|R)$.

If the reads were error-free and each position in the genome was
sequenced equally likely, the probability of observing read $r$
would simply be $\Pr(r|A)=n_r/(2L)$, where $n_r$ is the number of 
occurrences of the read as a substring of the assembly $A$,
$L$ is the length of $A$, and thus $2L$ is the length of the two
strands combined.

Other way of looking at the error free model is to say that probability
of generating read from position $j$ is one if read exactly matches 
assembly at given position and zero otherwise. This can be extended
to account for sequencing errors. The probability of generating read
from position $j$ is a real number which represent likelihood of generating
that read from given position. This value mainly depends on the number
of differences between read and assembly at the specific position.

Formally we define $p_{r, j}$ as probability of generating read $r$ from a sequence
that ends at position $j$. Then the probability of generating read $r$ can be computed as:

$$\Pr(r|A) = \frac{\sum_j p_{r,j}^{forward} + \sum_j p_{r,j}^{reverse}}{2L}$$ 

The individual probabilities $p_{r,j}$ can be computed via dynamic programing, where
we define $T[x,y]$ as a probability of generating prefix of read of length $y$ from sequence
which ends at position $x$. Clearly $p_{r,j} = T[j, \ell]$, where $\ell$ is the length
of the read. Also $T[x,0] = 1$ for all $x$ and $T[0,y]= 0$ for all $y > 0$.
The other probabilities can be computed using following formula:
$$T[x,y] = T[x-1,y-1]\Pr(Subs(A[x], r[y])) + T[x,y-1]\Pr(Ins(r[y])) +
T[x-1,y]\Pr(Del(A[x]))$$

Where $A[x], r[y]$ represents bases at the assembly at position $x$ and in read at
position $y$ respectively and $Subs, Ins, Del$ represents events of substition, insertion
and deletion.

However this dynamic programming is too time consuming. 
In practice it is good enough to align reads to the assembly and to compute
likelihood from the alignments.
Given read $r$ and 
a set $S_r$ of a few best alignments of $r$ to
genome~$A$, as obtained by one of standard fast read alignment tools, the
probability of generating read $r$ can be estimated as:
\begin{equation}
\Pr(r|A)\approx \frac{\sum_{j\in S_r} R(s_j, m_j)}{2L},
\end{equation}
where $m_j$ is the number of matches in the $j$-th alignment, and
$s_j$ is the number of mismatches and indels implied by this alignment.

\paragraph{Paired reads.}
Likelihood model can accomodate also paired reads.
We assume that the insert size distribution in a set of reads $R$ 
can be modeled by the normal
distribution with known mean $\mu$ and standard deviation $\sigma$.
The probability of observing paired reads $r_1$ and $r_2$ 
can be estimated from the sets of alignments $S_{r_1}$ and $S_{r_2}$ as follows:

\begin{equation}
\Pr(r_1, r_2|A) \approx 
\frac{1}{2L}
\displaystyle\sum_{j_1 \in S_{r_1}} 
\displaystyle\sum_{j_2 \in S_{r_2}} 
R(s_{j_1}, m_{j_1}) R(s_{j_2}, m_{j_2})
\Pr(d(j_1, j_2)|\mu, \sigma)
\end{equation}
As before, $m_{j_i}$ and $s_{j_i}$ are the numbers of matches and
sequencing errors in alignment $j_i$ respectively, 
and $d(j_1,j_2)$ is the distance between the two alignments
as observed in the assembly. 
If alignments $j_1$ and $j_2$ are in two different contigs,
or on inconsistent strands, $\Pr(d(j_1, j_2)|\mu, \sigma)$ is zero.

\paragraph{Reads that have no good alignment to $A$.}
Some reads or read pairs do not align well to $A$, and as a result, their
probability $\Pr(r|A)$ is very low; our approximation by a set of
high-scoring alignments can even yield zero probability if set $S_r$
is empty.  Such extremely low probabilities then dominate the log
likelihood score.  \cite{Ghodsi2013} propose a method that assigns
such a read a score approximating the situation when the read would be
added as a new contig to the assembly. In practice this usually
means having lower bound on the probability of generating a read.
